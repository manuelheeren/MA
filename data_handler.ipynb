{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import time\n",
    "from enum import Enum\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Asset(Enum):\n",
    "    \"\"\"Trading assets with their specific properties\"\"\"\n",
    "    XAUUSD = {\n",
    "        \"name\": \"XAUUSD\",\n",
    "        \"files\": [\n",
    "            \"datasets/raw/XAUUSD/Nov23-Nov24/XAUUSD_1M_BID.csv\"\n",
    "        ]\n",
    "    }\n",
    "    BTCUSD = {\n",
    "        \"name\": \"BTCUSD\",\n",
    "        \"files\": [\n",
    "            \"datasets/raw/BTCUSD/Jan22-Oct22/BTCUSD_1M_BID_01.01.2022-31.10.2022.csv\",\n",
    "            \"datasets/raw/BTCUSD/Nov22-Oct23/BTCUSD_1M_BID_01.11.2022-31.10.2023.csv\",\n",
    "            \"datasets/raw/BTCUSD/Nov23-Oct24/BTCUSD_1M_BID_01.11.2023-01.11.2024.csv\"\n",
    "        ]\n",
    "    }\n",
    "    SPY = {\n",
    "        \"name\": \"SPY\",\n",
    "        \"files\": [\n",
    "            \"datasets/raw/SPY/Jan22-Jun22/SPY.USUSD_Candlestick_1_M_BID_01.01.2020-30.06.2020.csv\",\n",
    "            \"datasets/raw/SPY/Jul22-Dec22/SPY.USUSD_Candlestick_1_M_BID_01.07.2020-31.12.2020.csv\",\n",
    "            \"datasets/raw/SPY/Jan23-Jun23/SPY.USUSD_Candlestick_1_M_BID_01.01.2021-30.06.2021.csv\",\n",
    "            \"datasets/raw/SPY/Jul23-Dec23/SPY.USUSD_Candlestick_1_M_BID_01.07.2021-31.12.2021.csv\",\n",
    "            \"datasets/raw/SPY/Jan22-Jun22/SPY.USUSD_Candlestick_1_M_BID_01.01.2022-30.06.2022.csv\",\n",
    "            \"datasets/raw/SPY/Jul22-Dec22/SPY.USUSD_Candlestick_1_M_BID_01.07.2022-31.12.2022.csv\",\n",
    "            \"datasets/raw/SPY/Jan23-Jun23/SPY.USUSD_Candlestick_1_M_BID_01.01.2023-30.06.2023.csv\",\n",
    "            \"datasets/raw/SPY/Jul23-Dec23/SPY.USUSD_Candlestick_1_M_BID_01.07.2023-31.12.2023.csv\",\n",
    "            \"datasets/raw/SPY/Jan24-Jun24/SPY.USUSD_Candlestick_1_M_BID_01.01.2024-30.06.2024.csv\",\n",
    "            \"datasets/raw/SPY/Jul24-Nov24/SPY.USUSD_Candlestick_1_M_BID_01.07.2024-30.11.2024.csv\"\n",
    "        ]\n",
    "    }\n",
    "    WTI = {\n",
    "        \"name\": \"WTI\",\n",
    "        \"files\": [\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2020-30.06.2020.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2020-31.12.2020.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2021-30.06.2021.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2021-31.12.2021.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2022-30.06.2022.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2022-31.12.2022.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2023-30.06.2023.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2023-31.12.2023.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2024-30.06.2024.csv\",\n",
    "            \"datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2024-01.11.2024.csv\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    @property\n",
    "    def files(self) -> List[str]:\n",
    "        return self.value[\"files\"]\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.value[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\"Basic data handler with session handling\"\"\"\n",
    "    \n",
    "    # Standard market sessions for all assets\n",
    "    SESSIONS = {\n",
    "        'asian': (time(0, 0), time(8, 0)),\n",
    "        'london': (time(8, 0), time(16, 0)),\n",
    "        'us': (time(13, 0), time(21, 0))\n",
    "    }\n",
    "    \n",
    "    def process_asset_data(self, asset: Asset) -> None:\n",
    "        \"\"\"Process all data files for a given asset\"\"\"\n",
    "        logger.info(f\"Processing {asset.name} data...\")\n",
    "        \n",
    "        # Create processed directory\n",
    "        processed_dir = Path(f\"datasets/processed/{asset.name}\")\n",
    "        processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Load and combine all data files\n",
    "        dfs = []\n",
    "        for file_path in asset.files:\n",
    "            file_path = Path(file_path)\n",
    "            if not file_path.exists():\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert timestamp\n",
    "            df['timestamp'] = pd.to_datetime(\n",
    "                df['Gmt time'],\n",
    "                format='%d.%m.%Y %H:%M:%S.%f',\n",
    "                utc=True\n",
    "            )\n",
    "            \n",
    "            # Standardize column names\n",
    "            df.rename(columns={\n",
    "                'Open': 'open',\n",
    "                'High': 'high',\n",
    "                'Low': 'low',\n",
    "                'Close': 'close',\n",
    "                'Volume': 'volume'\n",
    "            }, inplace=True)\n",
    "            \n",
    "            dfs.append(df)\n",
    "        \n",
    "        if not dfs:\n",
    "            logger.error(f\"No valid data files found for {asset.name}\")\n",
    "            return\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        combined_df.drop_duplicates(subset=['timestamp'], keep='last', inplace=True)\n",
    "        combined_df.sort_values('timestamp', inplace=True)\n",
    "        \n",
    "        # Set timestamp as index\n",
    "        combined_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Process and validate data\n",
    "        self.processed_data = self._process_data(combined_df)\n",
    "        \n",
    "        # Save processed data\n",
    "        output_path = processed_dir / \"combined_data.csv\"\n",
    "        self.processed_data.to_csv(output_path)\n",
    "        logger.info(f\"Saved processed data to {output_path}\")\n",
    "        \n",
    "        # Print basic statistics\n",
    "        start_date = self.processed_data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.processed_data.index.max().strftime('%Y-%m-%d')\n",
    "        logger.info(f\"Data range: {start_date} to {end_date}\")\n",
    "        logger.info(f\"Total records: {len(self.processed_data)}\")\n",
    "    \n",
    "    def _process_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process and validate data with basic session marking\"\"\"\n",
    "        # Make a copy to avoid modifying the original\n",
    "        processed = df.copy()\n",
    "        \n",
    "        # Add date and time columns\n",
    "        processed['date'] = processed.index.date\n",
    "        processed['time'] = processed.index.time\n",
    "        \n",
    "        # Add session markers\n",
    "        for session_name, (start, end) in self.SESSIONS.items():\n",
    "            if start < end:  # Normal session\n",
    "                session_mask = (processed['time'] >= start) & (processed['time'] < end)\n",
    "            else:  # Session crosses midnight\n",
    "                session_mask = (processed['time'] >= start) | (processed['time'] < end)\n",
    "            \n",
    "            processed[f'{session_name}_session'] = session_mask\n",
    "        \n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_selected_assets(assets_to_process: List[str]) -> None:\n",
    "    \"\"\"Process only selected assets\"\"\"\n",
    "    handler = DataHandler()\n",
    "    \n",
    "    for asset_name in assets_to_process:\n",
    "        try:\n",
    "            asset = Asset[asset_name]\n",
    "            handler.process_asset_data(asset)\n",
    "        except KeyError:\n",
    "            logger.error(f\"Invalid asset name: {asset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {asset_name}: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 11:35:26,750 - INFO - Processing WTI data...\n",
      "2025-02-24 11:35:26,751 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2020-30.06.2020.csv\n",
      "2025-02-24 11:35:26,751 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2020-31.12.2020.csv\n",
      "2025-02-24 11:35:26,752 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2021-30.06.2021.csv\n",
      "2025-02-24 11:35:26,752 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2021-31.12.2021.csv\n",
      "2025-02-24 11:35:26,752 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2022-30.06.2022.csv\n",
      "2025-02-24 11:35:26,753 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2022-31.12.2022.csv\n",
      "2025-02-24 11:35:26,753 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2023-30.06.2023.csv\n",
      "2025-02-24 11:35:26,753 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2023-31.12.2023.csv\n",
      "2025-02-24 11:35:26,753 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.01.2024-30.06.2024.csv\n",
      "2025-02-24 11:35:26,754 - WARNING - File not found: datasets/raw/WTI/LIGHT.CMDUSD_Candlestick_1_M_BID_01.07.2024-01.11.2024.csv\n",
      "2025-02-24 11:35:26,754 - ERROR - No valid data files found for WTI\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Process only SPY data\n",
    "    process_selected_assets([\"WTI\"])\n",
    "    \n",
    "    # For all assets:\n",
    "    # process_selected_assets([asset.name for asset in Asset])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4131570aad7ff77d93c6be89cfb61a61d96547c15666b9d6a932bac1ad3bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
