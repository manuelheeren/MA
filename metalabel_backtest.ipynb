{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from new_strategy import Asset, BetSizingMethod, get_bet_sizing\n",
    "import nbimporter\n",
    "from backtest import Backtest\n",
    "from meta_strategy import MetaLabelingStrategy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ---------------------- MetaModelHandler ---------------------- #\n",
    "class MetaModelHandler:\n",
    "    def __init__(self):\n",
    "        self.long_model = None\n",
    "        self.short_model = None\n",
    "        self.long_scaler = None\n",
    "        self.short_scaler = None\n",
    "        self.feature_cols = []\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        trades_df: pd.DataFrame, \n",
    "        long_feature_cols: list, \n",
    "        short_feature_cols: list,\n",
    "        asset_name: str,\n",
    "        method_name: str\n",
    "    ):\n",
    "        self.long_feature_cols = long_feature_cols\n",
    "        self.short_feature_cols = short_feature_cols\n",
    "\n",
    "        trades_df = trades_df.dropna(subset=['meta_label'])\n",
    "\n",
    "        long_trades = trades_df[trades_df['direction'] == 'long'].dropna(subset=long_feature_cols)\n",
    "        short_trades = trades_df[trades_df['direction'] == 'short'].dropna(subset=short_feature_cols)\n",
    "\n",
    "        def preprocess(df, cols):\n",
    "            X = df[cols]\n",
    "            y = df['meta_label']\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            return X_scaled, y, scaler\n",
    "\n",
    "        X_long, y_long, self.long_scaler = preprocess(long_trades, long_feature_cols)\n",
    "        X_short, y_short, self.short_scaler = preprocess(short_trades, short_feature_cols)\n",
    "\n",
    "        def _sanity_check(X, y, label):\n",
    "            print(f\"\\nðŸ“Š Sanity Check for {label} dataset\")\n",
    "            print(\"  â†’ Shape:\", X.shape)\n",
    "            print(\"  â†’ NaNs in X:\", np.isnan(X).sum())\n",
    "            print(\"  â†’ All-zero columns:\", (X == 0).all(axis=0).sum())\n",
    "            print(\"  â†’ y balance:\", np.bincount(y.astype(int)) if len(np.unique(y)) == 2 else y.value_counts())\n",
    "\n",
    "        _sanity_check(X_long, y_long, \"LONG\")\n",
    "        _sanity_check(X_short, y_short, \"SHORT\")\n",
    "\n",
    "\n",
    "        # Tune long model\n",
    "        print(\"[Optuna] Tuning LONG model...\")\n",
    "        best_long_params = optimize_model(X_long, y_long, n_trials=50)\n",
    "        self.long_model = LGBMClassifier(**best_long_params)\n",
    "        self.long_model.fit(X_long, y_long)\n",
    "\n",
    "        # Tune short model\n",
    "        print(\"[Optuna] Tuning SHORT model...\")\n",
    "        best_short_params = optimize_model(X_short, y_short, n_trials=50)\n",
    "        self.short_model = LGBMClassifier(**best_short_params)\n",
    "        self.short_model.fit(X_short, y_short)\n",
    "\n",
    "        self.plot_feature_importance(self.long_model, long_feature_cols, \"Long Trades\")\n",
    "        self.plot_feature_importance(self.short_model, short_feature_cols, \"Short Trades\")\n",
    "\n",
    "                # Convert to DataFrame for SHAP\n",
    "        X_long_df = pd.DataFrame(X_long, columns=long_feature_cols)\n",
    "        X_short_df = pd.DataFrame(X_short, columns=short_feature_cols)\n",
    "\n",
    "        # Convert to DataFrame for SHAP\n",
    "        X_long_df = pd.DataFrame(X_long, columns=long_feature_cols)\n",
    "        X_short_df = pd.DataFrame(X_short, columns=short_feature_cols)\n",
    "\n",
    "        # Use self.long_model and self.short_model\n",
    "        self.plot_shap_values(self.long_model, X_long_df, long_feature_cols, \"Long\", asset_name, method_name)\n",
    "        self.plot_shap_values(self.short_model, X_short_df, short_feature_cols, \"Short\", asset_name, method_name)\n",
    "\n",
    "    def plot_feature_importance(self, model, feature_names, title):\n",
    "        importance = model.feature_importances_\n",
    "        sorted_idx = importance.argsort()[::-1]\n",
    "        sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "        sorted_importance = importance[sorted_idx]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(sorted_names, sorted_importance)\n",
    "        plt.title(f\"ðŸ” Feature Importance â€” {title}\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_shap_values(self, model, X, feature_names, title, asset_name, method_name):\n",
    "\n",
    "        plt.close()\n",
    "        plt.style.use('default')\n",
    "\n",
    "        print(f\"[SHAP] Generating plot for: {title}\")\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        shap.plots.bar(shap_values, max_display=len(feature_names), show=False)\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        fig.suptitle(\n",
    "            f\"SHAP Feature Importance â€” {title.capitalize()} â€” {asset_name.upper()} â€” {method_name.upper()}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "        # Create output directory\n",
    "        output_dir = \"results_metalabel/shap\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Format filename with asset + method + long/short\n",
    "        safe_title = title.lower().replace(\" \", \"_\")\n",
    "        filename = f\"shap_{asset_name.lower()}_{method_name.lower()}_{safe_title}.png\"\n",
    "        full_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        plt.savefig(full_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[SHAP] Saved to {full_path}\")\n",
    "\n",
    "        # Compute mean absolute SHAP values per feature\n",
    "        mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "        shap_summary_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'mean_abs_shap_value': mean_abs_shap\n",
    "        }).sort_values(by='mean_abs_shap_value', ascending=False)\n",
    "\n",
    "        # Save the summary\n",
    "        summary_filename = f\"shap_summary_{asset_name.lower()}_{method_name.lower()}_{title.lower()}.csv\"\n",
    "        summary_path = os.path.join(\"results_metalabel/shap\", summary_filename)\n",
    "        shap_summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"[SHAP] Summary CSV saved to {summary_path}\")\n",
    "\n",
    "    def is_trade_approved(self, features: dict, direction: str, threshold: float = 0.5) -> bool:\n",
    "        if direction == 'long':\n",
    "            feature_list = self.long_feature_cols\n",
    "            model = self.long_model\n",
    "            scaler = self.long_scaler\n",
    "        else:\n",
    "            feature_list = self.short_feature_cols\n",
    "            model = self.short_model\n",
    "            scaler = self.short_scaler\n",
    "\n",
    "        cleaned = {}\n",
    "        for k in feature_list:\n",
    "            val = features.get(k, 0)\n",
    "            if pd.isna(val) or val in [np.inf, -np.inf]:\n",
    "                cleaned[k] = 0\n",
    "            else:\n",
    "                cleaned[k] = val\n",
    "\n",
    "        df = pd.DataFrame([cleaned])[feature_list]\n",
    "        X = scaler.transform(df)\n",
    "        prob = model.predict_proba(X)[0, 1]\n",
    "\n",
    "        print(f\"[MetaModel] Direction: {direction}, Prob: {prob:.3f}, Threshold: {threshold}, Approved: {prob >= threshold}\")\n",
    "        return prob >= threshold\n",
    "\n",
    "def train_meta_model(train_df: pd.DataFrame, long_feature_cols: list, short_feature_cols: list,asset, method) -> MetaModelHandler:\n",
    "    # Shift rolling metrics to avoid lookahead bias\n",
    "    rolling_cols = [\n",
    "        'rolling_f1', 'rolling_accuracy', 'rolling_precision', 'rolling_recall',\n",
    "        'n_total_seen', 'n_window_obs'\n",
    "    ]\n",
    "    for col in rolling_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df.groupby('session')[col].shift(1)\n",
    "    meta_model = MetaModelHandler()\n",
    "    meta_model.train(train_df, long_feature_cols, short_feature_cols,asset.value, method.value)\n",
    "    return meta_model\n",
    "\n",
    "def optimize_model(X, y, n_trials=150):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 64),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        score = cross_val_score(model, X, y, cv=tscv, scoring=make_scorer(f1_score)).mean()\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"âœ… Best parameters:\", study.best_params)\n",
    "    print(\"ðŸ“ˆ Best F1 score:\", study.best_value)\n",
    "    return study.best_params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4131570aad7ff77d93c6be89cfb61a61d96547c15666b9d6a932bac1ad3bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
