{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from new_strategy import Asset, BetSizingMethod, get_bet_sizing\n",
    "import nbimporter\n",
    "from backtest import Backtest\n",
    "from meta_strategy import MetaLabelingStrategy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import binom\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ---------------------- MetaModelHandler ---------------------- #\n",
    "class MetaModelHandler:\n",
    "    def __init__(self):\n",
    "        self.long_model = None\n",
    "        self.short_model = None\n",
    "        self.long_scaler = None\n",
    "        self.short_scaler = None\n",
    "        self.feature_cols = []\n",
    "\n",
    "    def train(self, trades_df, long_feature_cols, short_feature_cols, asset_name, method_name):\n",
    "\n",
    "        self.long_feature_cols = long_feature_cols\n",
    "        self.short_feature_cols = short_feature_cols\n",
    "\n",
    "        trades_df = trades_df.dropna(subset=['meta_label'])\n",
    "\n",
    "        # Remove any 'set' column if it exists (we're not using train/val split anymore)\n",
    "        if 'set' in trades_df.columns:\n",
    "            trades_df = trades_df.drop('set', axis=1)\n",
    "\n",
    "        # Prepare direction-specific data\n",
    "        long_trades = trades_df[trades_df['direction'] == 'long'].dropna(subset=long_feature_cols)\n",
    "        short_trades = trades_df[trades_df['direction'] == 'short'].dropna(subset=short_feature_cols)\n",
    "\n",
    "        # =================== LONG MODEL ===================\n",
    "        X_long = long_trades[long_feature_cols]\n",
    "        y_long = long_trades[\"meta_label\"]\n",
    "\n",
    "\n",
    "        # Fit scaler on all long data\n",
    "        X_long_scaled = X_long.values\n",
    "        self.long_scaler = None\n",
    "\n",
    "        self._sanity_check(X_long_scaled, y_long, \"LONG\")\n",
    "\n",
    "        print(\"[Optuna] Tuning LONG model...\")\n",
    "        best_long_params = self.optimize_model_cv(X_long_scaled, y_long, n_trials=50)\n",
    "\n",
    "        # Train final model with best parameters\n",
    "        lgbm_long = LGBMClassifier(**best_long_params)\n",
    "\n",
    "        # Use CalibratedClassifierCV with cross-validation (this is the key change)\n",
    "    \n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        calibrated_long = CalibratedClassifierCV(\n",
    "            lgbm_long, \n",
    "            method='isotonic',  # As recommended in the paper\n",
    "            cv=tscv  # Use time series cross-validation instead of 'prefit'\n",
    "        )\n",
    "\n",
    "        # Fit the calibrated model (this will do CV internally)\n",
    "        calibrated_long.fit(X_long_scaled, y_long)\n",
    "\n",
    "        # Fit raw model for evaluation purposes\n",
    "        lgbm_long.fit(X_long_scaled, y_long)\n",
    "\n",
    "        self.long_model = calibrated_long\n",
    "        self.raw_long_model = lgbm_long\n",
    "\n",
    "        self.evaluate_calibration_cv(\n",
    "            calibrated_model=calibrated_long,\n",
    "            X=X_long_scaled,\n",
    "            y=y_long,\n",
    "            label=\"LONG\"\n",
    "        )\n",
    "\n",
    "        X_long_df = pd.DataFrame(X_long_scaled, columns=self.long_feature_cols)\n",
    "\n",
    "        self.plot_shap_values(\n",
    "            model=lgbm_long,\n",
    "            X=X_long,\n",
    "            feature_names=self.long_feature_cols,\n",
    "            title=\"Long\",\n",
    "            asset_name=asset_name,\n",
    "            method_name=method_name\n",
    "        )\n",
    "\n",
    "        # =================== SHORT MODEL ===================\n",
    "        X_short = short_trades[short_feature_cols]\n",
    "        y_short = short_trades[\"meta_label\"]\n",
    "\n",
    "        # Fit scaler on all short data\n",
    "        X_short_scaled = X_short.values\n",
    "        self.short_scaler = None\n",
    "\n",
    "        self._sanity_check(X_short_scaled, y_short, \"SHORT\")\n",
    "\n",
    "        print(\"[Optuna] Tuning SHORT model...\")\n",
    "        best_short_params = self.optimize_model_cv(X_short_scaled, y_short, n_trials=50)\n",
    "\n",
    "        # Train final model with best parameters\n",
    "        lgbm_short = LGBMClassifier(**best_short_params)\n",
    "\n",
    "        # Use CalibratedClassifierCV with cross-validation\n",
    "        calibrated_short = CalibratedClassifierCV(\n",
    "            lgbm_short, \n",
    "            method='isotonic',\n",
    "            cv=tscv\n",
    "        )\n",
    "\n",
    "        # Fit the calibrated model\n",
    "        calibrated_short.fit(X_short_scaled, y_short)\n",
    "\n",
    "        # Fit raw model for evaluation purposes\n",
    "        lgbm_short.fit(X_short_scaled, y_short)\n",
    "\n",
    "        self.short_model = calibrated_short\n",
    "        self.raw_short_model = lgbm_short\n",
    "\n",
    "        self.evaluate_calibration_cv(\n",
    "            calibrated_model=calibrated_short,\n",
    "            X=X_short_scaled,\n",
    "            y=y_short,\n",
    "            label=\"SHORT\"\n",
    "        )\n",
    "\n",
    "        X_short_df = pd.DataFrame(X_short_scaled, columns=self.short_feature_cols)\n",
    "\n",
    "        self.plot_shap_values(\n",
    "            model=lgbm_short,\n",
    "            X=X_short,\n",
    "            feature_names=self.short_feature_cols,\n",
    "            title=\"Short\",\n",
    "            asset_name=asset_name,\n",
    "            method_name=method_name\n",
    "        )\n",
    "\n",
    "    def optimize_model_cv(self, X, y, n_trials=150):\n",
    "        \"\"\"Optimize model hyperparameters using cross-validation\"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 64),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1,\n",
    "                \"verbosity\": -1\n",
    "            }\n",
    "\n",
    "            model = LGBMClassifier(**params)\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            score = cross_val_score(model, X, y, cv=tscv, scoring=make_scorer(f1_score)).mean()\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"✅ Best parameters:\", study.best_params)\n",
    "        print(\"📈 Best F1 score:\", study.best_value)\n",
    "        return study.best_params\n",
    "\n",
    "    def evaluate_calibration(self, raw_model, calibrated_model, X, y, label):\n",
    "        cal_probs = calibrated_model.predict_proba(X)[:, 1]\n",
    "        print(f\"\\n📉 Calibration Evaluation — {label}\")\n",
    "        print(f\"  → Brier score (calibrated): {brier_score_loss(y, cal_probs):.4f}\")\n",
    "        \n",
    "        # ✅ Plot calibrated model calibration curve\n",
    "        self.plot_calibration_with_ci(y_true=y, y_probs=cal_probs,\n",
    "                                model_name=\"Calibrated Model\", label=label)\n",
    "\n",
    "        # Also evaluate raw model if provided\n",
    "        if raw_model is not None:\n",
    "            raw_probs = raw_model.predict_proba(X)[:, 1]\n",
    "            print(f\"  → Brier score (raw):        {brier_score_loss(y, raw_probs):.4f}\")\n",
    "            \n",
    "            # ✅ Plot raw model calibration curve\n",
    "            self.plot_calibration_with_ci(y_true=y, y_probs=raw_probs,\n",
    "                                    model_name=\"Raw Model\", label=label)\n",
    "    \n",
    "    def evaluate_calibration_cv(self, calibrated_model, X, y, label):\n",
    "        print(f\"\\n📉 Calibration Evaluation (CV) — {label}\")\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_probs_calibrated = []\n",
    "        cv_probs_raw = []\n",
    "        cv_true = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Handle different sklearn versions\n",
    "            base_est = getattr(calibrated_model, \"base_estimator\", None)\n",
    "            if base_est is None:\n",
    "                base_est = getattr(calibrated_model, \"estimator\", None)\n",
    "            base_model = LGBMClassifier(**base_est.get_params())\n",
    "            base_model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Get RAW predictions on validation fold\n",
    "            raw_probs = base_model.predict_proba(X_val_fold)[:, 1]\n",
    "            cv_probs_raw.extend(raw_probs)\n",
    "            \n",
    "            # Calibrate on validation fold (paper's method)\n",
    "            fold_calibrated = CalibratedClassifierCV(base_model, method='isotonic', cv='prefit')\n",
    "            fold_calibrated.fit(X_val_fold, y_val_fold)\n",
    "            \n",
    "            # Evaluate on same validation fold (paper's method - optimistic but consistent)\n",
    "            cal_probs = fold_calibrated.predict_proba(X_val_fold)[:, 1]\n",
    "            cv_probs_calibrated.extend(cal_probs)\n",
    "            \n",
    "            cv_true.extend(y_val_fold)\n",
    "        \n",
    "        cv_probs_calibrated = np.array(cv_probs_calibrated)\n",
    "        cv_probs_raw = np.array(cv_probs_raw)\n",
    "        cv_true = np.array(cv_true)\n",
    "        \n",
    "        # Calculate Brier scores (will be optimistic for calibrated model)\n",
    "        brier_calibrated = brier_score_loss(cv_true, cv_probs_calibrated)\n",
    "        brier_raw = brier_score_loss(cv_true, cv_probs_raw)\n",
    "        print(f\"  → Brier score (raw):        {brier_raw:.4f}\")\n",
    "        print(f\"  → Brier score (calibrated): {brier_calibrated:.4f}\")\n",
    "        print(f\"  → Improvement:              {brier_raw - brier_calibrated:.4f}\")\n",
    "        \n",
    "        # Plot comparison (calibrated will look very good)\n",
    "        self.plot_calibration_comparison(\n",
    "            y_true=cv_true, \n",
    "            y_probs_raw=cv_probs_raw,\n",
    "            y_probs_calibrated=cv_probs_calibrated,\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    \n",
    "    def plot_calibration_comparison(self, y_true, y_probs_raw, y_probs_calibrated, label='LONG', n_bins=10):\n",
    "        \"\"\"Plot both raw and calibrated predictions on the same chart\"\"\"\n",
    "        try:\n",
    "            print(f\"📊 Creating calibration comparison plot - {label}\")\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_probs_raw = np.array(y_probs_raw)\n",
    "            y_probs_calibrated = np.array(y_probs_calibrated)\n",
    "            \n",
    "            if len(y_true) < 10:\n",
    "                print(f\"⚠️ Too few samples: {len(y_true)}\")\n",
    "                return\n",
    "            \n",
    "            actual_bins = min(10, len(y_true) // 10)\n",
    "            if actual_bins < 2:\n",
    "                actual_bins = 2\n",
    "            \n",
    "            # Get calibration curves for both\n",
    "            frac_pos_raw, mean_pred_raw = calibration_curve(y_true, y_probs_raw, n_bins=actual_bins)\n",
    "            frac_pos_cal, mean_pred_cal = calibration_curve(y_true, y_probs_calibrated, n_bins=actual_bins)\n",
    "            \n",
    "            # Calculate Brier scores\n",
    "            brier_raw = np.mean((y_probs_raw - y_true) ** 2)\n",
    "            brier_cal = np.mean((y_probs_calibrated - y_true) ** 2)\n",
    "            \n",
    "            # Create comparison plot\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            # Plot both curves\n",
    "            ax.plot(mean_pred_raw, frac_pos_raw, 'o-', \n",
    "                    linewidth=2, markersize=8, label=f'Raw Model (Brier: {brier_raw:.4f})', color='red')\n",
    "            ax.plot(mean_pred_cal, frac_pos_cal, 's-', \n",
    "                    linewidth=2, markersize=8, label=f'Calibrated Model (Brier: {brier_cal:.4f})', color='blue')\n",
    "            \n",
    "            # Perfect calibration reference line\n",
    "            ax.plot([0, 1], [0, 1], '--', color='gray', label='Perfect Calibration')\n",
    "            \n",
    "            # Labels and formatting\n",
    "            ax.set_xlabel('Mean Predicted Probability')\n",
    "            ax.set_ylabel('Fraction of Positives') \n",
    "            ax.set_title(f'Calibration Comparison - {label}\\nImprovement: {brier_raw - brier_cal:.4f}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✅ Calibration comparison plot created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Calibration comparison plot failed: {e}\")\n",
    "            try:\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def _sanity_check(self, X, y, label):\n",
    "        \"\"\"Sanity check for data quality\"\"\"\n",
    "        print(f\"\\n📊 Sanity Check for {label} dataset\")\n",
    "        print(\"  → Shape:\", X.shape)\n",
    "        print(\"  → NaNs in X:\", np.isnan(X).sum())\n",
    "        print(\"  → All-zero columns:\", (X == 0).all(axis=0).sum())\n",
    "        print(\"  → y balance:\", np.bincount(y.astype(int)) if len(np.unique(y)) == 2 else y.value_counts())\n",
    "                    \n",
    "    def plot_shap_values(self, model, X, feature_names, title, asset_name, method_name):\n",
    "            \n",
    "        plt.close()\n",
    "        plt.style.use('default')\n",
    "\n",
    "        print(f\"[SHAP] Generating plot for: {title}\")\n",
    "            \n",
    "        # If using CalibratedClassifierCV, get the base estimator\n",
    "        if hasattr(model, \"base_estimator_\"):\n",
    "            model = model.base_estimator_\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot SHAP bar chart\n",
    "        shap.plots.bar(shap_values, max_display=len(feature_names), show=False)\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        fig.suptitle(\n",
    "            f\"SHAP Feature Importance — {title.capitalize()} — {asset_name.upper()} — {method_name.upper()}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "        # Output directory\n",
    "        output_dir = \"results_metalabel/shap\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Construct filename\n",
    "        safe_title = title.lower().replace(\" \", \"_\")\n",
    "        filename = f\"shap_{asset_name.lower()}_{method_name.lower()}_{safe_title}.png\"\n",
    "        full_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save image\n",
    "        plt.savefig(full_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[SHAP] Saved to {full_path}\")\n",
    "\n",
    "        # Save mean absolute SHAP values summary\n",
    "        mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "        shap_summary_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'mean_abs_shap_value': mean_abs_shap\n",
    "        }).sort_values(by='mean_abs_shap_value', ascending=False)\n",
    "\n",
    "        summary_filename = f\"shap_summary_{asset_name.lower()}_{method_name.lower()}_{safe_title}.csv\"\n",
    "        summary_path = os.path.join(output_dir, summary_filename)\n",
    "        shap_summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"[SHAP] Summary CSV saved to {summary_path}\")    \n",
    "\n",
    "    def is_trade_approved(self, features: dict, direction: str, threshold: float = 0.3) -> bool:\n",
    "        if direction == 'long':\n",
    "            feature_list = self.long_feature_cols\n",
    "            model = self.long_model\n",
    "        else:\n",
    "            feature_list = self.short_feature_cols\n",
    "            model = self.short_model\n",
    "            \n",
    "\n",
    "        cleaned = {}\n",
    "        for k in feature_list:\n",
    "            val = features.get(k, 0)\n",
    "            if pd.isna(val) or val in [np.inf, -np.inf]:\n",
    "                cleaned[k] = 0\n",
    "            else:\n",
    "                cleaned[k] = val\n",
    "\n",
    "        df = pd.DataFrame([cleaned])[feature_list]\n",
    "        X = df.values  # Use raw values, no scaling\n",
    "        prob = model.predict_proba(X)[0, 1]\n",
    "\n",
    "        print(f\"[MetaModel] Direction: {direction}, Prob: {prob:.3f}, Threshold: {threshold}, Approved: {prob >= threshold}\")\n",
    "        return prob >= threshold\n",
    "\n",
    "    def plot_calibration_with_ci(self, y_true, y_probs, model_name='Calibrated Model', label='LONG', n_bins=10):\n",
    "        \"\"\"Simple replacement - no confidence intervals, just basic calibration plot\"\"\"\n",
    "        try:\n",
    "            print(f\"📊 Creating simple calibration plot for {model_name} - {label}\")\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            y_true = np.array(y_true)\n",
    "            y_probs = np.array(y_probs)\n",
    "            \n",
    "            # Basic checks\n",
    "            if len(y_true) < 10:\n",
    "                print(f\"⚠️ Too few samples: {len(y_true)}\")\n",
    "                return\n",
    "            \n",
    "            # Use fewer bins for reliability\n",
    "            actual_bins = min(5, len(y_true) // 10)\n",
    "            if actual_bins < 2:\n",
    "                actual_bins = 2\n",
    "            \n",
    "            # Get calibration curve\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_true, y_probs, n_bins=actual_bins\n",
    "            )\n",
    "            \n",
    "            # Calculate Brier score\n",
    "            brier = np.mean((y_probs - y_true) ** 2)\n",
    "            \n",
    "            # Create simple plot\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            \n",
    "            # Plot calibration line\n",
    "            ax.plot(mean_predicted_value, fraction_of_positives, 'o-', \n",
    "                    linewidth=2, markersize=8, label=f'{model_name}')\n",
    "            \n",
    "            # Perfect calibration reference line\n",
    "            ax.plot([0, 1], [0, 1], '--', color='gray', label='Perfect Calibration')\n",
    "            \n",
    "            # Labels and formatting\n",
    "            ax.set_xlabel('Mean Predicted Probability')\n",
    "            ax.set_ylabel('Fraction of Positives') \n",
    "            ax.set_title(f'Calibration Plot - {label} - {model_name}\\nBrier Score: {brier:.4f}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✅ Calibration plot created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Calibration plot failed: {e}\")\n",
    "            try:\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass    \n",
    "\n",
    "\n",
    "\"\"\"def train_meta_model(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    long_feature_cols: list,\n",
    "    short_feature_cols: list,\n",
    "    asset,\n",
    "    method\n",
    ") -> MetaModelHandler:\n",
    "    # Shift rolling metrics to avoid lookahead bias\n",
    "    rolling_cols = [\n",
    "        'rolling_f1', 'rolling_accuracy', 'rolling_precision', 'rolling_recall',\n",
    "        'n_total_seen', 'n_window_obs'\n",
    "    ]\n",
    "    for col in rolling_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df.groupby('session')[col].shift(1)\n",
    "    \n",
    "    combined_df = pd.concat([train_df, val_df])\n",
    "    split_index = len(train_df)  \n",
    "\n",
    "    meta_model = MetaModelHandler()\n",
    "    meta_model.train(train_df, long_feature_cols, short_feature_cols,asset.value, method.value,split_index)\n",
    "    return meta_model\"\"\"\n",
    "\n",
    "def optimize_model(X, y, n_trials=150):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 64),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        score = cross_val_score(model, X, y, cv=tscv, scoring=make_scorer(f1_score)).mean()\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"✅ Best parameters:\", study.best_params)\n",
    "    print(\"📈 Best F1 score:\", study.best_value)\n",
    "    return study.best_params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4131570aad7ff77d93c6be89cfb61a61d96547c15666b9d6a932bac1ad3bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
