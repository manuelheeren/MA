{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/manuelheeren/Documents/Masterarbeit/MA/metalabel_backtest.ipynb Zelle 1\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/manuelheeren/Documents/Masterarbeit/MA/metalabel_backtest.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/manuelheeren/Documents/Masterarbeit/MA/metalabel_backtest.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mimport\u001b[39;00m LGBMClassifier\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/manuelheeren/Documents/Masterarbeit/MA/metalabel_backtest.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/manuelheeren/Documents/Masterarbeit/MA/metalabel_backtest.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/manuelheeren/Documents/Masterarbeit/MA/metalabel_backtest.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/shap/__init__.py:91\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m     plots \u001b[39m=\u001b[39m UnsupportedModule()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m# other stuff :)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, links, utils  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mactions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_optimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m ActionOptimizer  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m approximate_interactions, sample  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/shap/datasets.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/datasets/__init__.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_svmlight_format_io\u001b[39;00m \u001b[39mimport\u001b[39;00m load_svmlight_files\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_svmlight_format_io\u001b[39;00m \u001b[39mimport\u001b[39;00m dump_svmlight_file\n\u001b[0;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_olivetti_faces\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_olivetti_faces\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_species_distributions\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_species_distributions\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_california_housing\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_california_housing\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/datasets/_olivetti_faces.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m \u001b[39mimport\u001b[39;00m makedirs, remove\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatlab\u001b[39;00m \u001b[39mimport\u001b[39;00m loadmat\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_data_home\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/__init__.py:105\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m# Fortran file support\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fortran\u001b[39;00m \u001b[39mimport\u001b[39;00m FortranFile, FortranEOFError, FortranFormattingError\n\u001b[0;32m--> 105\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmmio\u001b[39;00m \u001b[39mimport\u001b[39;00m mminfo, mmread, mmwrite\n\u001b[1;32m    106\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39midl\u001b[39;00m \u001b[39mimport\u001b[39;00m readsav\n\u001b[1;32m    107\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mharwell_boeing\u001b[39;00m \u001b[39mimport\u001b[39;00m hb_read, hb_write\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:978\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:647\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from new_strategy import Asset, BetSizingMethod, get_bet_sizing\n",
    "import nbimporter\n",
    "from backtest import Backtest\n",
    "from meta_strategy import MetaLabelingStrategy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ---------------------- MetaModelHandler ---------------------- #\n",
    "class MetaModelHandler:\n",
    "    def __init__(self):\n",
    "        self.long_model = None\n",
    "        self.short_model = None\n",
    "        self.long_scaler = None\n",
    "        self.short_scaler = None\n",
    "        self.feature_cols = []\n",
    "\n",
    "    #LightGBM\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        trades_df: pd.DataFrame, \n",
    "        long_feature_cols: list, \n",
    "        short_feature_cols: list\n",
    "    ):\n",
    "        self.long_feature_cols = long_feature_cols\n",
    "        self.short_feature_cols = short_feature_cols\n",
    "\n",
    "        trades_df = trades_df.dropna(subset=['meta_label'])\n",
    "\n",
    "        long_trades = trades_df[trades_df['direction'] == 'long'].dropna(subset=long_feature_cols)\n",
    "        short_trades = trades_df[trades_df['direction'] == 'short'].dropna(subset=short_feature_cols)\n",
    "\n",
    "        def preprocess(df, cols):\n",
    "            X = df[cols]\n",
    "            y = df['meta_label']\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            return X_scaled, y, scaler\n",
    "\n",
    "        X_long, y_long, self.long_scaler = preprocess(long_trades, long_feature_cols)\n",
    "        X_short, y_short, self.short_scaler = preprocess(short_trades, short_feature_cols)\n",
    "\n",
    "        self.long_model = LGBMClassifier(random_state=42)\n",
    "        self.short_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "        self.long_model.fit(X_long, y_long)\n",
    "        self.short_model.fit(X_short, y_short)\n",
    "\n",
    "        self.plot_feature_importance(self.long_model, long_feature_cols, \"Long Trades\")\n",
    "        self.plot_feature_importance(self.short_model, short_feature_cols, \"Short Trades\")\n",
    "\n",
    "                # Convert to DataFrame for SHAP\n",
    "        X_long_df = pd.DataFrame(X_long, columns=long_feature_cols)\n",
    "        X_short_df = pd.DataFrame(X_short, columns=short_feature_cols)\n",
    "\n",
    "        self.plot_shap_values(self.long_model, X_long_df, long_feature_cols, \"Long Trades\")\n",
    "        self.plot_shap_values(self.short_model, X_short_df, short_feature_cols, \"Short Trades\")\n",
    "\n",
    "\n",
    "    def plot_feature_importance(self, model, feature_names, title):\n",
    "        importance = model.feature_importances_\n",
    "        sorted_idx = importance.argsort()[::-1]\n",
    "        sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "        sorted_importance = importance[sorted_idx]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(sorted_names, sorted_importance)\n",
    "        plt.title(f\"🔍 Feature Importance — {title}\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_shap_values(self, model, X, feature_names, title):\n",
    "\n",
    "        plt.close()\n",
    "        plt.style.use('default')\n",
    "\n",
    "        print(f\"[SHAP] Generating plot for: {title}\")\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        shap.plots.bar(shap_values, max_display=len(feature_names))\n",
    "\n",
    "        plt.gcf().suptitle(f\"📊 SHAP Feature Importance — {title}\", fontsize=14)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Prevent title overlap\n",
    "\n",
    "\n",
    "    #XGBOOST    \n",
    "\n",
    "    \"\"\"def train(self, trades_df: pd.DataFrame, feature_cols: list):\n",
    "        self.feature_cols = feature_cols\n",
    "        trades_df = trades_df.dropna(subset=feature_cols + ['meta_label'])\n",
    "\n",
    "        long_trades = trades_df[trades_df['direction'] == 'long']\n",
    "        short_trades = trades_df[trades_df['direction'] == 'short']\n",
    "\n",
    "        def preprocess(df):\n",
    "            X = df[feature_cols]\n",
    "            y = df['meta_label']\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            return X_scaled, y, scaler\n",
    "\n",
    "        X_long, y_long, self.long_scaler = preprocess(long_trades)\n",
    "        X_short, y_short, self.short_scaler = preprocess(short_trades)\n",
    "\n",
    "        self.long_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        self.short_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        self.long_model.fit(X_long, y_long)\n",
    "        self.short_model.fit(X_short, y_short)\n",
    "\n",
    "        self.plot_feature_importance(self.long_model, feature_cols, \"Long Trades\")\n",
    "        self.plot_feature_importance(self.short_model, feature_cols, \"Short Trades\")\n",
    "\n",
    "    def plot_feature_importance(self, model, feature_names, title):\n",
    "        importance = model.feature_importances_\n",
    "        sorted_idx = importance.argsort()[::-1]\n",
    "        sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "        sorted_importance = importance[sorted_idx]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(sorted_names, sorted_importance)\n",
    "        plt.title(f\"🔍 Feature Importance — {title}\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "    #Logistic Reg\n",
    "\n",
    "    \"\"\"def train(self, trades_df: pd.DataFrame, feature_cols: list):\n",
    "        self.feature_cols = feature_cols\n",
    "        trades_df = trades_df.dropna(subset=feature_cols + ['meta_label'])\n",
    "\n",
    "        long_trades = trades_df[trades_df['direction'] == 'long']\n",
    "        short_trades = trades_df[trades_df['direction'] == 'short']\n",
    "\n",
    "        def preprocess(df):\n",
    "            X = df[feature_cols]\n",
    "            y = df['meta_label']\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            return X_scaled, y, scaler\n",
    "\n",
    "        X_long, y_long, self.long_scaler = preprocess(long_trades)\n",
    "        X_short, y_short, self.short_scaler = preprocess(short_trades)\n",
    "\n",
    "        self.long_model = CalibratedClassifierCV(LogisticRegression(), method='sigmoid').fit(X_long, y_long)\n",
    "        self.short_model = CalibratedClassifierCV(LogisticRegression(), method='sigmoid').fit(X_short, y_short)\n",
    "\n",
    "        \n",
    "        self.long_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        self.short_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        self.long_model.fit(X_long, y_long)\n",
    "        self.short_model.fit(X_short, y_short)\"\"\"\n",
    "        \n",
    "\n",
    "\n",
    "    def is_trade_approved(self, features: dict, direction: str, threshold: float = 0.5) -> bool:\n",
    "        if direction == 'long':\n",
    "            feature_list = self.long_feature_cols\n",
    "            model = self.long_model\n",
    "            scaler = self.long_scaler\n",
    "        else:\n",
    "            feature_list = self.short_feature_cols\n",
    "            model = self.short_model\n",
    "            scaler = self.short_scaler\n",
    "\n",
    "        cleaned = {}\n",
    "        for k in feature_list:\n",
    "            val = features.get(k, 0)\n",
    "            if pd.isna(val) or val in [np.inf, -np.inf]:\n",
    "                cleaned[k] = 0\n",
    "            else:\n",
    "                cleaned[k] = val\n",
    "\n",
    "        df = pd.DataFrame([cleaned])[feature_list]\n",
    "        X = scaler.transform(df)\n",
    "        prob = model.predict_proba(X)[0, 1]\n",
    "\n",
    "        print(f\"[MetaModel] Direction: {direction}, Prob: {prob:.3f}, Threshold: {threshold}, Approved: {prob >= threshold}\")\n",
    "        return prob >= threshold\n",
    "\n",
    "\n",
    "def train_meta_model(train_df: pd.DataFrame, long_feature_cols: list, short_feature_cols: list) -> MetaModelHandler:\n",
    "    # Shift rolling metrics to avoid lookahead bias\n",
    "    rolling_cols = [\n",
    "        'rolling_f1', 'rolling_accuracy', 'rolling_precision', 'rolling_recall',\n",
    "        'n_total_seen', 'n_window_obs'\n",
    "    ]\n",
    "    for col in rolling_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df.groupby('session')[col].shift(1)\n",
    "    meta_model = MetaModelHandler()\n",
    "    meta_model.train(train_df, long_feature_cols, short_feature_cols)\n",
    "    return meta_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4131570aad7ff77d93c6be89cfb61a61d96547c15666b9d6a932bac1ad3bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
