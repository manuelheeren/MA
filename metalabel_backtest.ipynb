{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from new_strategy import Asset, BetSizingMethod, get_bet_sizing\n",
    "import nbimporter\n",
    "from backtest import Backtest\n",
    "from meta_strategy import MetaLabelingStrategy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import binom\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ---------------------- MetaModelHandler ---------------------- #\n",
    "class MetaModelHandler:\n",
    "    def __init__(self):\n",
    "        self.long_model = None\n",
    "        self.short_model = None\n",
    "        self.long_scaler = None\n",
    "        self.short_scaler = None\n",
    "        self.feature_cols = []\n",
    "\n",
    "    def train(self, trades_df, long_feature_cols, short_feature_cols, asset_name, method_name):\n",
    "\n",
    "        self.long_feature_cols = long_feature_cols\n",
    "        self.short_feature_cols = short_feature_cols\n",
    "\n",
    "        trades_df = trades_df.dropna(subset=['meta_label'])\n",
    "\n",
    "        # Remove any 'set' column if it exists (we're not using train/val split anymore)\n",
    "        if 'set' in trades_df.columns:\n",
    "            trades_df = trades_df.drop('set', axis=1)\n",
    "\n",
    "        # Prepare direction-specific data\n",
    "        long_trades = trades_df[trades_df['direction'] == 'long'].dropna(subset=long_feature_cols)\n",
    "        short_trades = trades_df[trades_df['direction'] == 'short'].dropna(subset=short_feature_cols)\n",
    "\n",
    "        # =================== LONG MODEL ===================\n",
    "        X_long = long_trades[long_feature_cols]\n",
    "        y_long = long_trades[\"meta_label\"]\n",
    "\n",
    "\n",
    "        # Fit scaler on all long data\n",
    "        X_long_scaled = X_long.values\n",
    "        self.long_scaler = None\n",
    "\n",
    "        self._sanity_check(X_long_scaled, y_long, \"LONG\")\n",
    "\n",
    "        print(\"[Optuna] Tuning LONG model...\")\n",
    "       # best_long_params = self.optimize_model_cv(X_long_scaled, y_long, n_trials=50)\n",
    "        best_long_params  = self.optimize_model_cv(X_long_scaled,  y_long,  n_trials=50, n_splits=5, metric=\"auc\")\n",
    "\n",
    "\n",
    "        # Train final model with best parameters\n",
    "        lgbm_long = LGBMClassifier(**best_long_params)\n",
    "\n",
    "        # === LONG: OOF SHAP ===\n",
    "        asset_series_long = long_trades[\"asset\"] if \"asset\" in long_trades.columns else None\n",
    "        global_imp_long, portable_imp_long, per_fold_imp_long = self.oof_shap_importance_lgb(\n",
    "            X_long.values, y_long,\n",
    "            feature_names=self.long_feature_cols,\n",
    "            base_params=best_long_params,\n",
    "            n_splits=5,\n",
    "            asset_series=asset_series_long,\n",
    "            embargo=0  # bump if your windows overlap near split boundary\n",
    "        )\n",
    "\n",
    "        # Save artifacts (same files as before, but OOF-based)\n",
    "        self.save_oof_shap_artifacts(global_imp_long, asset_name, method_name, title=\"long\")\n",
    "\n",
    "        # Optional: select features by share of OOF SHAP (e.g., >= 5%)\n",
    "        glong = global_imp_long.copy()\n",
    "        glong[\"pct_of_total\"] = glong[\"mean_abs_shap\"] / glong[\"mean_abs_shap\"].sum()\n",
    "        selected_long = glong.loc[glong[\"pct_of_total\"] >= 0.05, \"feature\"].tolist()\n",
    "        print(\"[LONG] Selected features (pct >= 5%):\", selected_long)\n",
    "\n",
    "\n",
    "        # Fit raw model for evaluation purposes\n",
    "        lgbm_long.fit(X_long_scaled, y_long)\n",
    "\n",
    "        self.long_model = lgbm_long\n",
    "        self.raw_long_model = lgbm_long\n",
    "        self.long_raw_metrics, self.long_oof, self.long_lift = self._evaluate_raw_model(\n",
    "        X_long_scaled, y_long, lgbm_long, label=\"LONG\", n_splits=5\n",
    ")          \n",
    "        raw_probs = self.long_oof  # Use the OOF predictions from _evaluate_raw_model\n",
    "        self.plot_calibration_with_ci(y_true=y_long, y_probs=self.long_oof, \n",
    "                             model_name=\"Raw LONG Model (OOF)\", label=\"LONG\")\n",
    "\n",
    "        X_long_df = pd.DataFrame(X_long_scaled, columns=self.long_feature_cols)\n",
    "\n",
    "        \"\"\"self.plot_shap_values(\n",
    "            model=lgbm_long,\n",
    "            X=X_long,\n",
    "            feature_names=self.long_feature_cols,\n",
    "            title=\"Long\",\n",
    "            asset_name=asset_name,\n",
    "            method_name=method_name\n",
    "        )\"\"\"\n",
    "\n",
    "        # =================== SHORT MODEL ===================\n",
    "        X_short = short_trades[short_feature_cols]\n",
    "        y_short = short_trades[\"meta_label\"]\n",
    "\n",
    "        # Fit scaler on all short data\n",
    "        X_short_scaled = X_short.values\n",
    "        self.short_scaler = None\n",
    "\n",
    "        self._sanity_check(X_short_scaled, y_short, \"SHORT\")\n",
    "\n",
    "        print(\"[Optuna] Tuning SHORT model...\")\n",
    "        #best_short_params = self.optimize_model_cv(X_short_scaled, y_short, n_trials=50)\n",
    "        best_short_params = self.optimize_model_cv(X_short_scaled, y_short, n_trials=50, n_splits=5, metric=\"auc\")\n",
    "\n",
    "\n",
    "        # Train final model with best parameters\n",
    "        lgbm_short = LGBMClassifier(**best_short_params)\n",
    "\n",
    "        # === SHORT: OOF SHAP ===\n",
    "        asset_series_short = short_trades[\"asset\"] if \"asset\" in short_trades.columns else None\n",
    "        global_imp_short, portable_imp_short, per_fold_imp_short = self.oof_shap_importance_lgb(\n",
    "            X_short.values, y_short,\n",
    "            feature_names=self.short_feature_cols,\n",
    "            base_params=best_short_params,\n",
    "            n_splits=5,\n",
    "            asset_series=asset_series_short,\n",
    "            embargo=0\n",
    "        )\n",
    "\n",
    "        # Save artifacts (same files as before, but OOF-based)\n",
    "        self.save_oof_shap_artifacts(global_imp_short, asset_name, method_name, title=\"short\")\n",
    "\n",
    "        gshort = global_imp_short.copy()\n",
    "        gshort[\"pct_of_total\"] = gshort[\"mean_abs_shap\"] / gshort[\"mean_abs_shap\"].sum()\n",
    "        selected_short = gshort.loc[gshort[\"pct_of_total\"] >= 0.05, \"feature\"].tolist()\n",
    "        print(\"[SHORT] Selected features (pct >= 5%):\", selected_short)\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "        # Fit raw model for evaluation purposes\n",
    "        lgbm_short.fit(X_short_scaled, y_short)\n",
    "\n",
    "        self.short_model = lgbm_short\n",
    "        self.raw_short_model = lgbm_short\n",
    "        self.short_raw_metrics, self.short_oof, self.short_lift = self._evaluate_raw_model(\n",
    "        X_short_scaled, y_short, lgbm_short, label=\"SHORT\", n_splits=5\n",
    "         )\n",
    "\n",
    "        raw_probs = self.short_oof  # Use OOF predictions\n",
    "        self.plot_calibration_with_ci(y_true=y_short, y_probs=self.short_oof,\n",
    "                                    model_name=\"Raw SHORT Model\", label=\"SHORT\")\n",
    "     \n",
    "\n",
    "        X_short_df = pd.DataFrame(X_short_scaled, columns=self.short_feature_cols)\n",
    "\n",
    "        \"\"\"self.plot_shap_values(\n",
    "            model=lgbm_short,\n",
    "            X=X_short,\n",
    "            feature_names=self.short_feature_cols,\n",
    "            title=\"Short\",\n",
    "            asset_name=asset_name,\n",
    "            method_name=method_name\n",
    "        )\"\"\"\n",
    "\n",
    "\n",
    "    def evaluate_calibration(self, raw_model, calibrated_model, X, y, label):\n",
    "        cal_probs = calibrated_model.predict_proba(X)[:, 1]\n",
    "        print(f\"\\n📉 Calibration Evaluation — {label}\")\n",
    "        print(f\"  → Brier score (calibrated): {brier_score_loss(y, cal_probs):.4f}\")\n",
    "        \n",
    "        # ✅ Plot calibrated model calibration curve\n",
    "        self.plot_calibration_with_ci(y_true=y, y_probs=cal_probs,\n",
    "                                model_name=\"Calibrated Model\", label=label)\n",
    "\n",
    "        # Also evaluate raw model if provided\n",
    "        if raw_model is not None:\n",
    "            raw_probs = raw_model.predict_proba(X)[:, 1]\n",
    "            print(f\"  → Brier score (raw):        {brier_score_loss(y, raw_probs):.4f}\")\n",
    "            \n",
    "            # ✅ Plot raw model calibration curve\n",
    "            self.plot_calibration_with_ci(y_true=y, y_probs=raw_probs,\n",
    "                                    model_name=\"Raw Model\", label=label)\n",
    "\n",
    "    def optimize_model_cv(self, X, y, n_trials=80, n_splits=5, metric=\"auc\", patience=150, seed=42):\n",
    "        \"\"\"\n",
    "        Hyperparameter search with TimeSeriesSplit + early stopping.\n",
    "        Supports boosting_type in {\"gbdt\",\"goss\"}.\n",
    "        metric: \"auc\" (recommended) or \"f1\".\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import lightgbm as lgb\n",
    "        import optuna\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        def objective(trial):\n",
    "            boosting = trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"goss\"])\n",
    "            params = {\n",
    "                \"objective\": \"binary\",\n",
    "                \"boosting_type\": boosting,\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.08, log=True),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256, log=True),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 300),\n",
    "                \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "                \"n_estimators\": 4000,  # early stopping will cap this\n",
    "                \"random_state\": seed,\n",
    "                \"n_jobs\": -1,\n",
    "                \"verbosity\": -1,\n",
    "            }\n",
    "            if boosting == \"goss\":\n",
    "                params[\"top_rate\"] = trial.suggest_float(\"top_rate\", 0.1, 0.4)\n",
    "                params[\"other_rate\"] = trial.suggest_float(\"other_rate\", 0.0, 0.2)\n",
    "                params[\"subsample\"] = 1.0\n",
    "                params[\"subsample_freq\"] = 0\n",
    "            else:\n",
    "                params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "                params[\"subsample_freq\"] = trial.suggest_int(\"subsample_freq\", 0, 7)\n",
    "\n",
    "            scores, best_iters = [], []\n",
    "            for tr, va in tscv.split(X):\n",
    "                m = lgb.LGBMClassifier(**params)\n",
    "                y_tr = y.iloc[tr] if hasattr(y, \"iloc\") else y[tr]\n",
    "                y_va = y.iloc[va] if hasattr(y, \"iloc\") else y[va]\n",
    "                m.fit(\n",
    "                    X[tr], y_tr,\n",
    "                    eval_set=[(X[va], y_va)],\n",
    "                    eval_metric=(\"auc\" if metric == \"auc\" else \"binary_logloss\"),\n",
    "                    callbacks=[lgb.early_stopping(patience, verbose=False)],\n",
    "                )\n",
    "                p = m.predict_proba(X[va])[:, 1]\n",
    "                fold_score = roc_auc_score(y_va, p) if metric == \"auc\" else f1_score(y_va, (p >= 0.5).astype(int))\n",
    "                scores.append(fold_score)\n",
    "                best_iters.append(getattr(m, \"best_iteration_\", params[\"n_estimators\"]))\n",
    "\n",
    "            trial.set_user_attr(\"mean_best_iter\", int(np.mean(best_iters)))\n",
    "            return float(np.mean(scores))\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "        best = study.best_params\n",
    "        mean_best_iter = int(study.best_trial.user_attrs.get(\"mean_best_iter\", 800))\n",
    "        best.update({\n",
    "            \"objective\": \"binary\",\n",
    "            \"n_estimators\": max(100, int(mean_best_iter * 1.1)),\n",
    "            \"random_state\": seed,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbosity\": -1,\n",
    "        })\n",
    "        if best.get(\"boosting_type\") == \"goss\":\n",
    "            best.setdefault(\"subsample\", 1.0)\n",
    "            best.setdefault(\"subsample_freq\", 0)\n",
    "\n",
    "        print(\"✅ Best parameters:\", best)\n",
    "        print(f\"📈 Best CV {metric.upper()}: {study.best_value:.4f} (mean best_iter ≈ {mean_best_iter})\")\n",
    "        return best\n",
    "\n",
    "    \n",
    "    def evaluate_calibration_cv(self, calibrated_model, X, y, label):\n",
    "        print(f\"\\n📉 Calibration Evaluation (CV) — {label}\")\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_probs_calibrated = []\n",
    "        cv_probs_raw = []\n",
    "        cv_true = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Handle different sklearn versions\n",
    "            base_est = getattr(calibrated_model, \"base_estimator\", None)\n",
    "            if base_est is None:\n",
    "                base_est = getattr(calibrated_model, \"estimator\", None)\n",
    "            base_model = LGBMClassifier(**base_est.get_params())\n",
    "            base_model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Get RAW predictions on validation fold\n",
    "            raw_probs = base_model.predict_proba(X_val_fold)[:, 1]\n",
    "            cv_probs_raw.extend(raw_probs)\n",
    "            \n",
    "            # Calibrate on validation fold (paper's method)\n",
    "            fold_calibrated = CalibratedClassifierCV(base_model, method='isotonic', cv='prefit')\n",
    "            fold_calibrated.fit(X_val_fold, y_val_fold)\n",
    "            \n",
    "            # Evaluate on same validation fold (paper's method - optimistic but consistent)\n",
    "            cal_probs = fold_calibrated.predict_proba(X_val_fold)[:, 1]\n",
    "            cv_probs_calibrated.extend(cal_probs)\n",
    "            \n",
    "            cv_true.extend(y_val_fold)\n",
    "        \n",
    "        cv_probs_calibrated = np.array(cv_probs_calibrated)\n",
    "        cv_probs_raw = np.array(cv_probs_raw)\n",
    "        cv_true = np.array(cv_true)\n",
    "        \n",
    "        # Calculate Brier scores (will be optimistic for calibrated model)\n",
    "        brier_calibrated = brier_score_loss(cv_true, cv_probs_calibrated)\n",
    "        brier_raw = brier_score_loss(cv_true, cv_probs_raw)\n",
    "        print(f\"  → Brier score (raw):        {brier_raw:.4f}\")\n",
    "        print(f\"  → Brier score (calibrated): {brier_calibrated:.4f}\")\n",
    "        print(f\"  → Improvement:              {brier_raw - brier_calibrated:.4f}\")\n",
    "        \n",
    "        # Plot comparison (calibrated will look very good)\n",
    "        self.plot_calibration_comparison(\n",
    "            y_true=cv_true, \n",
    "            y_probs_raw=cv_probs_raw,\n",
    "            y_probs_calibrated=cv_probs_calibrated,\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    \n",
    "    def plot_calibration_comparison(self, y_true, y_probs_raw, y_probs_calibrated, label='LONG', n_bins=10):\n",
    "        \"\"\"Plot both raw and calibrated predictions on the same chart\"\"\"\n",
    "        try:\n",
    "            print(f\"📊 Creating calibration comparison plot - {label}\")\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_probs_raw = np.array(y_probs_raw)\n",
    "            y_probs_calibrated = np.array(y_probs_calibrated)\n",
    "            \n",
    "            if len(y_true) < 10:\n",
    "                print(f\"⚠️ Too few samples: {len(y_true)}\")\n",
    "                return\n",
    "            \n",
    "            actual_bins = min(10, len(y_true) // 10)\n",
    "            if actual_bins < 2:\n",
    "                actual_bins = 2\n",
    "            \n",
    "            # Get calibration curves for both\n",
    "            frac_pos_raw, mean_pred_raw = calibration_curve(y_true, y_probs_raw, n_bins=actual_bins)\n",
    "            frac_pos_cal, mean_pred_cal = calibration_curve(y_true, y_probs_calibrated, n_bins=actual_bins)\n",
    "            \n",
    "            # Calculate Brier scores\n",
    "            brier_raw = np.mean((y_probs_raw - y_true) ** 2)\n",
    "            brier_cal = np.mean((y_probs_calibrated - y_true) ** 2)\n",
    "            \n",
    "            # Create comparison plot\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            # Plot both curves\n",
    "            ax.plot(mean_pred_raw, frac_pos_raw, 'o-', \n",
    "                    linewidth=2, markersize=8, label=f'Raw Model (Brier: {brier_raw:.4f})', color='red')\n",
    "            ax.plot(mean_pred_cal, frac_pos_cal, 's-', \n",
    "                    linewidth=2, markersize=8, label=f'Calibrated Model (Brier: {brier_cal:.4f})', color='blue')\n",
    "            \n",
    "            # Perfect calibration reference line\n",
    "            ax.plot([0, 1], [0, 1], '--', color='gray', label='Perfect Calibration')\n",
    "            \n",
    "            # Labels and formatting\n",
    "            ax.set_xlabel('Mean Predicted Probability')\n",
    "            ax.set_ylabel('Fraction of Positives') \n",
    "            ax.set_title(f'Calibration Comparison - {label}\\nImprovement: {brier_raw - brier_cal:.4f}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✅ Calibration comparison plot created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Calibration comparison plot failed: {e}\")\n",
    "            try:\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def _sanity_check(self, X, y, label):\n",
    "        \"\"\"Sanity check for data quality\"\"\"\n",
    "        print(f\"\\n📊 Sanity Check for {label} dataset\")\n",
    "        print(\"  → Shape:\", X.shape)\n",
    "        print(\"  → NaNs in X:\", np.isnan(X).sum())\n",
    "        print(\"  → All-zero columns:\", (X == 0).all(axis=0).sum())\n",
    "        print(\"  → y balance:\", np.bincount(y.astype(int)) if len(np.unique(y)) == 2 else y.value_counts())\n",
    "                    \n",
    "    def plot_shap_values(self, model, X, feature_names, title, asset_name, method_name):\n",
    "    \n",
    "        plt.close()\n",
    "        plt.style.use('default')\n",
    "\n",
    "        print(f\"[SHAP] Generating plot for: {title}\")\n",
    "            \n",
    "        # If using CalibratedClassifierCV, get the base estimator\n",
    "        if hasattr(model, \"base_estimator_\"):\n",
    "            model = model.base_estimator_\n",
    "\n",
    "        try:\n",
    "            # Create SHAP explainer with proper settings\n",
    "            explainer = shap.TreeExplainer(model, feature_perturbation=\"interventional\")\n",
    "            \n",
    "            # Use smaller sample for speed and stability\n",
    "            sample_size = min(500, len(X))\n",
    "            sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "            X_sample = X.iloc[sample_idx] if hasattr(X, 'iloc') else X[sample_idx]\n",
    "            \n",
    "            shap_values = explainer(X_sample)\n",
    "\n",
    "            # Plot SHAP bar chart\n",
    "            shap.plots.bar(shap_values, max_display=len(feature_names), show=False)\n",
    "\n",
    "            fig = plt.gcf()\n",
    "            fig.suptitle(\n",
    "                f\"SHAP Feature Importance — {title.capitalize()} — {asset_name.upper()} — {method_name.upper()}\",\n",
    "                fontsize=14\n",
    "            )\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "            # Output directory\n",
    "            output_dir = \"results_metalabel/shap\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Construct filename\n",
    "            safe_title = title.lower().replace(\" \", \"_\")\n",
    "            filename = f\"shap_{asset_name.lower()}_{method_name.lower()}_{safe_title}.png\"\n",
    "            full_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Save image\n",
    "            plt.savefig(full_path, dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"[SHAP] Saved to {full_path}\")\n",
    "\n",
    "            # Save mean absolute SHAP values summary\n",
    "            mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "            shap_summary_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'mean_abs_shap_value': mean_abs_shap\n",
    "            }).sort_values(by='mean_abs_shap_value', ascending=False)\n",
    "\n",
    "            summary_filename = f\"shap_summary_{asset_name.lower()}_{method_name.lower()}_{safe_title}.csv\"\n",
    "            summary_path = os.path.join(output_dir, summary_filename)\n",
    "            shap_summary_df.to_csv(summary_path, index=False)\n",
    "            print(f\"[SHAP] Summary CSV saved to {summary_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[SHAP] Error generating SHAP plot: {e}\")\n",
    "            print(f\"[SHAP] Skipping SHAP analysis for {title}\")\n",
    "    \n",
    "\n",
    "    def is_trade_approved(self, features: dict, direction: str, threshold: float = 0.3) -> bool:\n",
    "        if direction == 'long':\n",
    "            feature_list = self.long_feature_cols\n",
    "            model = self.long_model\n",
    "        else:\n",
    "            feature_list = self.short_feature_cols\n",
    "            model = self.short_model\n",
    "            \n",
    "\n",
    "        cleaned = {}\n",
    "        for k in feature_list:\n",
    "            val = features.get(k, 0)\n",
    "            if pd.isna(val) or val in [np.inf, -np.inf]:\n",
    "                cleaned[k] = 0\n",
    "            else:\n",
    "                cleaned[k] = val\n",
    "\n",
    "        df = pd.DataFrame([cleaned])[feature_list]\n",
    "        X = df.values  # Use raw values, no scaling\n",
    "        prob = model.predict_proba(X)[0, 1]\n",
    "\n",
    "        print(f\"[MetaModel] Direction: {direction}, Prob: {prob:.3f}, Threshold: {threshold}, Approved: {prob >= threshold}\")\n",
    "        return prob >= threshold\n",
    "\n",
    "    def plot_calibration_with_ci(self, y_true, y_probs, model_name='Calibrated Model', label='LONG', n_bins=10):\n",
    "        \"\"\"Simple replacement - no confidence intervals, just basic calibration plot\"\"\"\n",
    "        try:\n",
    "            print(f\"📊 Creating simple calibration plot for {model_name} - {label}\")\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            y_true = np.array(y_true)\n",
    "            y_probs = np.array(y_probs)\n",
    "            \n",
    "            # Basic checks\n",
    "            if len(y_true) < 10:\n",
    "                print(f\"⚠️ Too few samples: {len(y_true)}\")\n",
    "                return\n",
    "            \n",
    "            # Use fewer bins for reliability\n",
    "            actual_bins = min(5, len(y_true) // 10)\n",
    "            if actual_bins < 2:\n",
    "                actual_bins = 2\n",
    "            \n",
    "            # Get calibration curve\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_true, y_probs, n_bins=actual_bins\n",
    "            )\n",
    "            \n",
    "            # Calculate Brier score\n",
    "            brier = np.mean((y_probs - y_true) ** 2)\n",
    "            \n",
    "            # Create simple plot\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            \n",
    "            # Plot calibration line\n",
    "            ax.plot(mean_predicted_value, fraction_of_positives, 'o-', \n",
    "                    linewidth=2, markersize=8, label=f'{model_name}')\n",
    "            \n",
    "            # Perfect calibration reference line\n",
    "            ax.plot([0, 1], [0, 1], '--', color='gray', label='Perfect Calibration')\n",
    "            \n",
    "            # Labels and formatting\n",
    "            ax.set_xlabel('Mean Predicted Probability')\n",
    "            ax.set_ylabel('Fraction of Positives') \n",
    "            ax.set_title(f'Calibration Plot - {label} - {model_name}\\nBrier Score: {brier:.4f}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✅ Calibration plot created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Calibration plot failed: {e}\")\n",
    "            try:\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass    \n",
    "    def oof_shap_importance_lgb(\n",
    "        self,\n",
    "        X, y,\n",
    "        feature_names,\n",
    "        base_params,\n",
    "        n_splits=5,\n",
    "        asset_series=None,  # kept for compatibility; not used\n",
    "        embargo=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Honest OOF SHAP for LightGBM:\n",
    "        - For each TimeSeriesSplit fold, fit on train and compute SHAP (pred_contrib) on validation rows only.\n",
    "        - Returns: (global_df, None, per_fold_df)\n",
    "        global_df/per_fold_df columns: ['feature','mean_abs_shap', ...]\n",
    "        \"\"\"\n",
    "        import numpy as np, pandas as pd\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        from lightgbm import LGBMClassifier\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        n, p = X.shape\n",
    "        shap_vals = np.zeros((n, p), dtype=float)  # only validation rows will be filled\n",
    "        in_val = np.zeros(n, dtype=bool)\n",
    "        fold_id = np.full(n, -1, dtype=int)\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        y_arr = y.iloc if hasattr(y, \"iloc\") else y\n",
    "\n",
    "        for f, (tr, va) in enumerate(tscv.split(X)):\n",
    "            # optional embargo to reduce boundary leakage\n",
    "            if embargo > 0:\n",
    "                tr = tr[tr < (va[0] - embargo)]\n",
    "                va = va[embargo:] if len(va) > embargo else va[:0]\n",
    "            if len(tr) == 0 or len(va) == 0:\n",
    "                continue\n",
    "\n",
    "            m = LGBMClassifier(**base_params)\n",
    "            m.fit(X[tr], y_arr[tr])\n",
    "\n",
    "            explainer = shap.TreeExplainer(m, feature_perturbation=\"interventional\")  # model_output defaults to raw\n",
    "            sv = explainer(X[va])                          # shap.Explanation\n",
    "            contrib = sv.values         # (n_val, p+1) last col is bias\n",
    "            shap_vals[va] = contrib               # drop bias col\n",
    "\n",
    "            in_val[va] = True\n",
    "            fold_id[va] = f\n",
    "\n",
    "        # keep only OOF rows\n",
    "        shap_oof = shap_vals[in_val]\n",
    "        folds_oof = fold_id[in_val]\n",
    "        abs_shap = np.abs(shap_oof)\n",
    "\n",
    "        # per-fold mean(|SHAP|)\n",
    "        import pandas as pd\n",
    "        per_fold = []\n",
    "        for f in np.unique(folds_oof):\n",
    "            idx = (folds_oof == f)\n",
    "            mean_abs = abs_shap[idx].mean(axis=0)\n",
    "            per_fold.append(pd.DataFrame({\"feature\": feature_names, \"mean_abs_shap\": mean_abs, \"fold\": f}))\n",
    "        per_fold_df = pd.concat(per_fold, ignore_index=True).sort_values(\n",
    "            [\"fold\", \"mean_abs_shap\"], ascending=[True, False]\n",
    "        )\n",
    "\n",
    "        # global = mean across folds\n",
    "        global_df = (\n",
    "            per_fold_df.groupby(\"feature\", as_index=False)[\"mean_abs_shap\"]\n",
    "            .mean()\n",
    "            .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        return global_df, None, per_fold_df\n",
    "    \n",
    "    def save_oof_shap_artifacts(self, imp_df, asset_name, method_name, title, outdir=\"results_metalabel/shap\"):\n",
    "        \"\"\"\n",
    "        Save CSV + bar plot (same naming as old code) based on OOF SHAP.\n",
    "        - Expects imp_df with columns: ['feature','mean_abs_shap']\n",
    "        - Writes:\n",
    "            shap_summary_{asset}_{method}_{title}.csv  (feature, mean_abs_shap_value, pct_of_total)\n",
    "            shap_{asset}_{method}_{title}.png\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        df = imp_df.copy()\n",
    "        total = df[\"mean_abs_shap\"].sum()\n",
    "        df[\"pct_of_total\"] = df[\"mean_abs_shap\"] / (total if total > 0 else 1.0)\n",
    "        df = df.sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "        df.rename(columns={\"mean_abs_shap\": \"mean_abs_shap_value\"}, inplace=True)\n",
    "\n",
    "        # CSV (backward-compatible schema)\n",
    "        csv_path = os.path.join(\n",
    "            outdir, f\"shap_summary_{asset_name.lower()}_{method_name.lower()}_{title.lower()}.csv\"\n",
    "        )\n",
    "        df[[\"feature\", \"mean_abs_shap_value\", \"pct_of_total\"]].to_csv(csv_path, index=False)\n",
    "        print(f\"[SHAP-OOF] Summary CSV saved to {csv_path}\")\n",
    "\n",
    "        # PNG bar plot (like shap.plots.bar)\n",
    "        max_display = len(df)\n",
    "        fig_h = max(4, 0.35 * max_display)\n",
    "        fig, ax = plt.subplots(figsize=(10, fig_h))\n",
    "        ax.barh(df[\"feature\"].iloc[:max_display][::-1], df[\"mean_abs_shap_value\"].iloc[:max_display][::-1])\n",
    "        ax.set_xlabel(\"Mean |SHAP| (OOF)\")\n",
    "        ax.set_ylabel(\"Feature\")\n",
    "        ax.set_title(\n",
    "            f\"SHAP Feature Importance — {title.capitalize()} — {asset_name.upper()} — {method_name.upper()} (OOF)\"\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "\n",
    "        png_path = os.path.join(\n",
    "            outdir, f\"shap_{asset_name.lower()}_{method_name.lower()}_{title.lower()}.png\"\n",
    "        )\n",
    "        plt.savefig(png_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[SHAP-OOF] Plot saved to {png_path}\")\n",
    "\n",
    "\n",
    "    def _oof_scores(self, X, y, base_params, n_splits=5):\n",
    "        \"\"\"Out-of-fold probabilities with TimeSeriesSplit (honest, no leakage).\"\"\"\n",
    "        import numpy as np\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        from lightgbm import LGBMClassifier\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        oof = np.zeros(len(y), dtype=float)\n",
    "        for tr, va in tscv.split(X):\n",
    "            m = LGBMClassifier(**base_params)\n",
    "            m.fit(X[tr], y.iloc[tr] if hasattr(y, \"iloc\") else y[tr])\n",
    "            oof[va] = m.predict_proba(X[va])[:, 1]\n",
    "        return oof\n",
    "\n",
    "    def _decile_lift(self, probs, y, n=10):\n",
    "        \"\"\"Hit rate and lift per score decile (top=decile 1).\"\"\"\n",
    "        import numpy as np, pandas as pd\n",
    "        order = np.argsort(-probs)\n",
    "        cuts = np.array_split(order, n)\n",
    "        base = float(y.mean())\n",
    "        rows = []\n",
    "        for i, idx in enumerate(cuts, 1):\n",
    "            rate = float(y.iloc[idx].mean() if hasattr(y, \"iloc\") else y[idx].mean())\n",
    "            lift = (rate / base) if base > 0 else float(\"nan\")\n",
    "            rows.append((i, len(idx), rate, lift))\n",
    "        return pd.DataFrame(rows, columns=[\"decile\",\"count\",\"pos_rate\",\"lift\"])\n",
    "\n",
    "    def _evaluate_raw_model(self, X, y, fitted_model, label=\"LONG\", n_splits=5):\n",
    "        \"\"\"Print in-sample spread + OOF metrics + top-decile lift.\"\"\"\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "\n",
    "        # In-sample spread (sanity check)\n",
    "        p_in = fitted_model.predict_proba(X)[:, 1]\n",
    "        print(f\"\\n[{label}] score spread: min {p_in.min():.3f} | max {p_in.max():.3f} | \"\n",
    "              f\"std {p_in.std():.3f} | q05 {np.quantile(p_in,.05):.3f} | q95 {np.quantile(p_in,.95):.3f}\")\n",
    "\n",
    "        # Honest OOF metrics\n",
    "        base_params = fitted_model.get_params()\n",
    "        oof = self._oof_scores(X, y, base_params, n_splits=n_splits)\n",
    "        metrics = {\n",
    "            \"AUC_OOF\": float(roc_auc_score(y, oof)),\n",
    "            \"PR_AUC_OOF\": float(average_precision_score(y, oof)),\n",
    "            \"Brier_OOF\": float(brier_score_loss(y, oof)),\n",
    "            \"LogLoss_OOF\": float(log_loss(y, oof, eps=1e-15)),\n",
    "            \"BaseRate\": float(np.mean(y)),\n",
    "        }\n",
    "        print(f\"[{label}] OOF metrics → {metrics}\")\n",
    "\n",
    "        # Decile lift (top few lines)\n",
    "        lift_df = self._decile_lift(oof, y, n=10)\n",
    "        print(f\"[{label}] Top deciles (OOF):\")\n",
    "        print(lift_df.head(3).to_string(index=False))\n",
    "        return metrics, oof, lift_df\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4131570aad7ff77d93c6be89cfb61a61d96547c15666b9d6a932bac1ad3bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
