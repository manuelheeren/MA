{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import datetime\n",
    "from io import StringIO\n",
    "from new_strategy import Asset\n",
    "from pathlib import Path\n",
    "from new_strategy import Asset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ Data time range: 2020-01-01 23:00:00 to 2024-12-31 21:58:00\n",
      "üìä Number of rows: 1772913\n",
      "‚úÖ All required OHLC columns are present.\n",
      "\n",
      "‚è≥ Total expected timestamps: 2629379\n",
      "‚ùå Missing timestamps: 856466\n",
      "üîç Sample missing timestamps: DatetimeIndex(['2020-01-02 03:33:00', '2020-01-02 04:26:00',\n",
      "               '2020-01-02 21:21:00', '2020-01-02 22:00:00',\n",
      "               '2020-01-02 22:01:00', '2020-01-02 22:02:00',\n",
      "               '2020-01-02 22:03:00', '2020-01-02 22:04:00',\n",
      "               '2020-01-02 22:05:00', '2020-01-02 22:06:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "üìÜ Missing Timestamps by Day:\n",
      "            missing_count    weekday\n",
      "2020-01-02             63   Thursday\n",
      "2020-01-03            120     Friday\n",
      "2020-01-04           1440   Saturday\n",
      "2020-01-05           1380     Sunday\n",
      "2020-01-06             60     Monday\n",
      "2020-01-07             60    Tuesday\n",
      "2020-01-08             60  Wednesday\n",
      "2020-01-09             60   Thursday\n",
      "2020-01-10            120     Friday\n",
      "2020-01-11           1440   Saturday\n",
      "\n",
      "üìä Total Missing Timestamps by Weekday:\n",
      "weekday\n",
      "Saturday     375840\n",
      "Sunday       350260\n",
      "Friday        53031\n",
      "Monday        25746\n",
      "Thursday      17989\n",
      "Wednesday     17264\n",
      "Tuesday       16336\n",
      "Name: missing_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "file_path = 'data/raw/XAUUSD/combined_data.csv.csv'   # Replace with your actual file path\n",
    "timestamp_column = 'timestamp'  # Adjust if your timestamp column is named differently\n",
    "expected_freq = '1min'         # Use '1min' or '1D' depending on your data\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine 'Date' and 'Timestamp' into a single datetime\n",
    "df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Timestamp'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Drop the original Date and Timestamp columns (optional)\n",
    "df.drop(columns=['Date', 'Timestamp'], inplace=True)\n",
    "\n",
    "# === CHECK TIME RANGE ===\n",
    "start = df.index.min()\n",
    "end = df.index.max()\n",
    "print(f\"\\nüìÖ Data time range: {start} to {end}\")\n",
    "print(f\"üìä Number of rows: {len(df)}\")\n",
    "\n",
    "# === CHECK REQUIRED COLUMNS ===\n",
    "required_cols = ['Open', 'High', 'Low', 'Close']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required OHLC columns are present.\")\n",
    "\n",
    "# === CHECK FOR MISSING TIMESTAMPS ===\n",
    "full_index = pd.date_range(start=start, end=end, freq=expected_freq)\n",
    "missing_timestamps = full_index.difference(df.index)\n",
    "\n",
    "print(f\"\\n‚è≥ Total expected timestamps: {len(full_index)}\")\n",
    "print(f\"‚ùå Missing timestamps: {len(missing_timestamps)}\")\n",
    "\n",
    "if not missing_timestamps.empty:\n",
    "    print(\"üîç Sample missing timestamps:\", missing_timestamps[:10])\n",
    "\n",
    "# Optional: Save missing timestamps to file\n",
    "# pd.Series(missing_timestamps).to_csv(\"missing_timestamps.csv\", index=False)\n",
    "\n",
    "# === GROUP MISSING TIMESTAMPS BY DATE ===\n",
    "missing_dates = pd.Series(missing_timestamps).dt.normalize()  # strip to date only\n",
    "missing_day_counts = missing_dates.value_counts().sort_index()\n",
    "\n",
    "# Add weekday info\n",
    "missing_day_info = pd.DataFrame({\n",
    "    'missing_count': missing_day_counts,\n",
    "})\n",
    "missing_day_info['weekday'] = missing_day_info.index.day_name()\n",
    "\n",
    "# Group by weekday to see overall pattern\n",
    "weekday_summary = missing_day_info.groupby('weekday')['missing_count'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nüìÜ Missing Timestamps by Day:\")\n",
    "print(missing_day_info.head(10))  # Show a few specific dates\n",
    "\n",
    "print(\"\\nüìä Total Missing Timestamps by Weekday:\")\n",
    "print(weekday_summary)\n",
    "\n",
    "# Optional: Save to CSV for review\n",
    "# missing_day_info.to_csv(\"missing_day_info.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear       879656\n",
      "Neutral    833390\n",
      "Bull       814182\n",
      "Name: regime_label, dtype: int64\n",
      "‚úÖ Saved processed data to: data/processed/BTCUSD/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "ASSET = Asset.BTCUSD\n",
    "asset_name = ASSET.value\n",
    "\n",
    "# Load the already merged minute-level data\n",
    "df = pd.read_csv(f\"data/raw/{asset_name}/combined_data.csv\", parse_dates=['timestamp'], index_col='timestamp')\n",
    "df.index = df.index.tz_localize(None)\n",
    "if 'ATR' in df.columns:\n",
    "    df.drop(columns='ATR', inplace=True)\n",
    "\n",
    "# === Ensure all days are present in index and forward-fill ===\n",
    "start_date = df.index.min().normalize()\n",
    "end_date = df.index.max().normalize()\n",
    "full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Add missing days with last known values to preserve continuity for rolling metrics\n",
    "df_daily_filled = df.resample('1D').agg({\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "}).reindex(full_date_range)\n",
    "\n",
    "df_daily_filled.ffill(inplace=True)\n",
    "\n",
    "# Use forward-filled daily OHLC to preserve rolling consistency\n",
    "daily_ohlc = df_daily_filled.copy()\n",
    "\n",
    "#t10y\n",
    "t10y = pd.read_csv(\"data/preprocessing_data/T10YIE.csv\", parse_dates=['observation_date'], index_col='observation_date')\n",
    "t10y.index = t10y.index.tz_localize(None)\n",
    "t10y.rename(columns={t10y.columns[0]: 't10yie'}, inplace=True)\n",
    "t10y = t10y.asfreq('D').ffill()\n",
    "t10y['t10yie'] = t10y['t10yie'].shift(1)\n",
    "daily_ohlc = daily_ohlc.merge(t10y, how='left', left_index=True, right_index=True)\n",
    "\n",
    "#CPI\n",
    "cpi = pd.read_csv(\"data/preprocessing_data/CPIAUCSL.csv\", parse_dates=['observation_date'], index_col='observation_date')\n",
    "cpi.index = cpi.index.tz_localize(None)\n",
    "cpi.rename(columns={cpi.columns[0]: 'cpiaucsl'}, inplace=True)\n",
    "cpi['cpiaucsl'] = cpi['cpiaucsl'].shift(1)\n",
    "cpi = cpi.asfreq('D').ffill()\n",
    "daily_ohlc = daily_ohlc.merge(cpi, how='left', left_index=True, right_index=True)\n",
    "\n",
    "#VIX\n",
    "vix = pd.read_csv(\"data/preprocessing_data/VIX_History.csv\", parse_dates=['date'], index_col='date')\n",
    "vix.index = vix.index.tz_localize(None)\n",
    "vix['vix_close'] = vix['close'].shift(1)\n",
    "vix = vix[['vix_close']]\n",
    "daily_ohlc = daily_ohlc.merge(vix, how='left', left_index=True, right_index=True)\n",
    "\n",
    "#DTW\n",
    "dollar = pd.read_csv(\"data/preprocessing_data/DTWEXBGS.csv\", parse_dates=['observation_date'], index_col='observation_date')\n",
    "dollar.index = dollar.index.tz_localize(None)\n",
    "dollar.rename(columns={dollar.columns[0]: 'dtwexbgs'}, inplace=True)\n",
    "dollar['dtwexbgs'] = dollar['dtwexbgs'].shift(1)\n",
    "dollar = dollar.asfreq('D').ffill()\n",
    "daily_ohlc = daily_ohlc.merge(dollar, how='left', left_index=True, right_index=True)\n",
    "\n",
    "#DGS10\n",
    "dgs10 = pd.read_csv(\"data/preprocessing_data/DGS10.csv\", parse_dates=['observation_date'], index_col='observation_date')\n",
    "dgs10.index = dgs10.index.tz_localize(None)\n",
    "dgs10.rename(columns={dgs10.columns[0]: 'dgs10'}, inplace=True)\n",
    "dgs10['dgs10'] = dgs10['dgs10'].shift(1)\n",
    "dgs10 = dgs10.asfreq('D').ffill()\n",
    "daily_ohlc = daily_ohlc.merge(dgs10, how='left', left_index=True, right_index=True)\n",
    "\n",
    "daily_ohlc['prev_close'] = daily_ohlc['close'].shift(1)\n",
    "# Calculate True Range (TR)\n",
    "daily_ohlc['true_range'] = daily_ohlc.apply(\n",
    "    lambda row: max(\n",
    "        row['high'] - row['low'],\n",
    "        abs(row['high'] - row['prev_close']),\n",
    "        abs(row['low'] - row['prev_close'])\n",
    "    ) if pd.notnull(row['prev_close']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#Calculate 14-day ATR (excluding current day via shift)\n",
    "daily_ohlc['atr_14'] = daily_ohlc['true_range'].rolling(window=14).mean().shift(1)\n",
    "\n",
    "#Calculate moving averages on daily close (also shifted)\n",
    "daily_ohlc['ma_14'] = daily_ohlc['close'].rolling(window=14).mean().shift(1)\n",
    "daily_ohlc['ma_30'] = daily_ohlc['close'].rolling(window=30).mean().shift(1)\n",
    "daily_ohlc['ma_100'] = daily_ohlc['close'].rolling(window=100).mean().shift(1)\n",
    "\n",
    "# Rolling max/min of the daily close\n",
    "daily_ohlc['max_14'] = daily_ohlc['close'].rolling(window=14).max().shift(1)\n",
    "daily_ohlc['min_14'] = daily_ohlc['close'].rolling(window=14).min().shift(1)\n",
    "\n",
    "daily_ohlc['max_30'] = daily_ohlc['close'].rolling(window=30).max().shift(1)\n",
    "daily_ohlc['min_30'] = daily_ohlc['close'].rolling(window=30).min().shift(1)\n",
    "\n",
    "daily_ohlc['max_100'] = daily_ohlc['close'].rolling(window=100).max().shift(1)\n",
    "daily_ohlc['min_100'] = daily_ohlc['close'].rolling(window=100).min().shift(1)\n",
    "\n",
    "\n",
    "\n",
    "# Cumulative max of close price\n",
    "daily_ohlc['static_peak'] = daily_ohlc['close'].cummax()\n",
    "\n",
    "# Static drawdown from that peak\n",
    "daily_ohlc['drawdown_static'] = (\n",
    "    (daily_ohlc['close'] - daily_ohlc['static_peak']) / daily_ohlc['static_peak']\n",
    ").shift(1)\n",
    "\n",
    "daily_ohlc['drawdown_30'] = (\n",
    "    (daily_ohlc['close'] - daily_ohlc['close'].rolling(window=30).max()) \n",
    "    / daily_ohlc['close'].rolling(window=30).max()\n",
    ").shift(1)\n",
    "\n",
    "# Forward-fill daily values into minute-level data\n",
    "df['daily_high'] = daily_ohlc['high'].reindex(df.index, method='ffill')\n",
    "df['daily_low'] = daily_ohlc['low'].reindex(df.index, method='ffill')\n",
    "df['daily_close'] = daily_ohlc['close'].reindex(df.index, method='ffill')\n",
    "df['true_range'] = daily_ohlc['true_range'].reindex(df.index, method='ffill')\n",
    "df['atr_14'] = daily_ohlc['atr_14'].reindex(df.index, method='ffill')\n",
    "df['ma_14'] = daily_ohlc['ma_14'].reindex(df.index, method='ffill')\n",
    "df['ma_30'] = daily_ohlc['ma_30'].reindex(df.index, method='ffill')\n",
    "df['ma_100'] = daily_ohlc['ma_100'].reindex(df.index, method='ffill')\n",
    "\n",
    "#Day and Weeknumber\n",
    "df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "df['week_number'] = df.index.isocalendar().week\n",
    "df['hour_of_day'] = df.index.hour\n",
    "\n",
    "df['max_price_14'] = daily_ohlc['max_14'].reindex(df.index, method='ffill')\n",
    "df['min_price_14'] = daily_ohlc['min_14'].reindex(df.index, method='ffill')\n",
    "\n",
    "df['max_price_30'] = daily_ohlc['max_30'].reindex(df.index, method='ffill')\n",
    "df['min_price_30'] = daily_ohlc['min_30'].reindex(df.index, method='ffill')\n",
    "\n",
    "df['max_price_100'] = daily_ohlc['max_100'].reindex(df.index, method='ffill')\n",
    "df['min_price_100'] = daily_ohlc['min_100'].reindex(df.index, method='ffill')\n",
    "\n",
    "#Drawdown\n",
    "df['drawdown_static'] = daily_ohlc['drawdown_static'].reindex(df.index, method='ffill')\n",
    "df['drawdown_30'] = daily_ohlc['drawdown_30'].reindex(df.index, method='ffill')\n",
    "\n",
    "# Forward-fill T10YIE into minute-level data\n",
    "df['t10yie'] = daily_ohlc['t10yie'].reindex(df.index, method='ffill')\n",
    "#FF CPI\n",
    "df['cpiaucsl'] = daily_ohlc['cpiaucsl'].reindex(df.index, method='ffill')\n",
    "#FF vix\n",
    "df['vix_close'] = daily_ohlc['vix_close'].reindex(df.index, method='ffill')\n",
    "#FF Dollar \n",
    "df['dtwexbgs'] = daily_ohlc['dtwexbgs'].reindex(df.index, method='ffill')\n",
    "#FF DGS10\n",
    "df['dgs10'] = daily_ohlc['dgs10'].reindex(df.index, method='ffill')\n",
    "\n",
    "# 1. Compute HMM Features on Daily Data\n",
    "daily_ohlc['return_raw'] = daily_ohlc['close'].pct_change()\n",
    "daily_ohlc['volatility_raw'] = (\n",
    "    daily_ohlc['close']\n",
    "    .rolling(window=10)\n",
    "    .apply(lambda x: np.mean((x - x.mean())**2))\n",
    ").shift(1)\n",
    "\n",
    "# === Shifted versions for use in model (to avoid lookahead bias) ===\n",
    "daily_ohlc['return'] = daily_ohlc['return_raw'].shift(1)\n",
    "daily_ohlc['volatility'] = daily_ohlc['volatility_raw'].shift(1)\n",
    "\n",
    "df['daily_return'] = daily_ohlc['return'].reindex(df.index, method='ffill')\n",
    "df['daily_volatility'] = daily_ohlc['volatility'].reindex(df.index, method='ffill')\n",
    "\n",
    "# 2. Drop NaNs (due to rolling/shift)\n",
    "hmm_features = daily_ohlc[['return_raw', 'volatility_raw']].dropna()\n",
    "\n",
    "# 3. Fit HMM on ALL data (no slicing)\n",
    "model = GaussianHMM(n_components=3, covariance_type='full', n_iter=75, random_state=42)\n",
    "model.fit(hmm_features)\n",
    "hmm_features['regime'] = model.predict(hmm_features)\n",
    "\n",
    "# 4. Label Regimes (Bull, Bear, Neutral) based on return means\n",
    "regime_means = hmm_features.groupby('regime')['return_raw'].mean().sort_values()\n",
    "regime_mapping = {\n",
    "    regime_means.index[0]: 'Bear',\n",
    "    regime_means.index[1]: 'Neutral',\n",
    "    regime_means.index[2]: 'Bull'\n",
    "}\n",
    "hmm_features['regime_label'] = hmm_features['regime'].map(regime_mapping)\n",
    "\n",
    "# 5. Merge regime info into daily_ohlc\n",
    "daily_ohlc['regime'] = hmm_features['regime']\n",
    "daily_ohlc['regime_label'] = hmm_features['regime_label']\n",
    "\n",
    "# 6. Forward-fill daily regime into minute-level df\n",
    "df['regime'] = daily_ohlc['regime'].reindex(df.index, method='ffill')\n",
    "df['regime_label'] = daily_ohlc['regime_label'].reindex(df.index, method='ffill')\n",
    "\n",
    "#calc avg 30 day return\n",
    "daily_ohlc['avg_return_30d'] = daily_ohlc['return_raw'].rolling(window=30).mean().shift(1)\n",
    "df['avg_return_30d'] = daily_ohlc['avg_return_30d'].reindex(df.index, method='ffill')\n",
    "#Optional: Inspect regime distribution\n",
    "print(df['regime_label'].value_counts())\n",
    "\n",
    "# Overwrite the original file\n",
    "df.index = df.index.tz_localize('UTC')\n",
    "output_path = f\"data/processed/{asset_name}/combined_data.csv\"\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path)\n",
    "print(f\"‚úÖ Saved processed data to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\n# === Settings ===\\nasset_name = \"WTI\"  # or \"WTI\"\\npath = f\"data/raw/{asset_name}/combined_data.csv\"\\n\\n# === Load the data ===\\ndf = pd.read_csv(path, parse_dates=[\\'timestamp\\'], index_col=\\'timestamp\\')\\ndf.index = df.index.tz_localize(None)\\n\\n# === Determine date range ===\\nstart_date = df.index.min().normalize()\\nend_date = df.index.max().normalize()\\nfull_range = pd.date_range(start=start_date, end=end_date, freq=\\'D\\')\\n\\n# === Actual calendar days in the data ===\\nactual_days = df.index.normalize().unique()\\nactual_days = pd.DatetimeIndex(actual_days)\\n\\n# === Find missing days ===\\nmissing_days = full_range.difference(actual_days)\\n\\n# === Output ===\\nprint(f\"üìÜ Start date: {start_date.date()}\")\\nprint(f\"üìÜ End date:   {end_date.date()}\")\\nprint(f\"üóìÔ∏è Total calendar days: {len(full_range)}\")\\nprint(f\"üìâ Missing days: {len(missing_days)}\\n\")\\n\\nif len(missing_days) > 0:\\n    print(\"üö´ Missing dates and their weekdays:\")\\n    for day in missing_days:\\n        print(f\" - {day.date()} ({day.strftime(\\'%A\\')})\")\\nelse:\\n    print(\"‚úÖ No missing dates in raw data.\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "\n",
    "# === Settings ===\n",
    "asset_name = \"WTI\"  # or \"WTI\"\n",
    "path = f\"data/raw/{asset_name}/combined_data.csv\"\n",
    "\n",
    "# === Load the data ===\n",
    "df = pd.read_csv(path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "df.index = df.index.tz_localize(None)\n",
    "\n",
    "# === Determine date range ===\n",
    "start_date = df.index.min().normalize()\n",
    "end_date = df.index.max().normalize()\n",
    "full_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# === Actual calendar days in the data ===\n",
    "actual_days = df.index.normalize().unique()\n",
    "actual_days = pd.DatetimeIndex(actual_days)\n",
    "\n",
    "# === Find missing days ===\n",
    "missing_days = full_range.difference(actual_days)\n",
    "\n",
    "# === Output ===\n",
    "print(f\"üìÜ Start date: {start_date.date()}\")\n",
    "print(f\"üìÜ End date:   {end_date.date()}\")\n",
    "print(f\"üóìÔ∏è Total calendar days: {len(full_range)}\")\n",
    "print(f\"üìâ Missing days: {len(missing_days)}\\n\")\n",
    "\n",
    "if len(missing_days) > 0:\n",
    "    print(\"üö´ Missing dates and their weekdays:\")\n",
    "    for day in missing_days:\n",
    "        print(f\" - {day.date()} ({day.strftime('%A')})\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing dates in raw data.\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4131570aad7ff77d93c6be89cfb61a61d96547c15666b9d6a932bac1ad3bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
